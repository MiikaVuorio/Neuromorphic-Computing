{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0820b0ad",
   "metadata": {},
   "source": [
    "# STDP learning for classifying touch input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64f9a9",
   "metadata": {},
   "source": [
    "Based on the architecture presented in Peter U. Diehl's seminal 2015 work: \"Unsupervised learning of digit recognition using spike-timing-dependent plasticity\", (https://www.frontiersin.org/articles/10.3389/fncom.2015.00099/full, https://github.com/peter-u-diehl/stdp-mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55960cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Increasing width of notebook, because of long lines\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fe7986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: brian2 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (2.5.2)\n",
      "Requirement already satisfied: setuptools>=24.2 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (65.6.3)\n",
      "Requirement already satisfied: sympy>=1.2 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (1.11.1)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (1.25.0)\n",
      "Requirement already satisfied: jinja2>=2.7 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (3.1.2)\n",
      "Requirement already satisfied: cython>=0.29 in c:\\users\\vuoriom3\\appdata\\roaming\\python\\python310\\site-packages (from brian2) (0.29.35)\n",
      "Requirement already satisfied: packaging in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (22.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from brian2) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2>=2.7->brian2) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vuoriom3\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.2->brian2) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#brian2 installation!\n",
    "!python -m pip install brian2\n",
    "#!conda install -c conda-forge brian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5413e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import statements\n",
    "import numpy as np\n",
    "import matplotlib.cm as cmap\n",
    "import time\n",
    "import os.path\n",
    "import scipy \n",
    "import pickle\n",
    "#import brian_no_units  #import it to deactivate unit checking --> This should NOT be done for testing/debugging \n",
    "import brian2 as b\n",
    "from struct import unpack\n",
    "from brian2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e39e035e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing random things\n",
    "19*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89982739",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([[0.50507, 0.44034, 0.46123, 0.67343, 0.35549],\n",
       "        [0.92397, 0.4919 , 1.     , 0.54114, 0.77173],\n",
       "        [0.71412, 0.41536, 0.31926, 0.48007, 0.23604],\n",
       "        [0.53695, 0.43384, 0.46149, 0.52328, 0.33612],\n",
       "        [0.8936 , 0.27796, 0.71448, 0.45855, 0.69858],\n",
       "        [0.60955, 0.32748, 0.5988 , 0.3855 , 0.     ],\n",
       "        [0.58787, 0.41138, 0.60814, 0.481  , 0.40345],\n",
       "        [0.59751, 0.56391, 0.84542, 0.57221, 0.42226],\n",
       "        [0.19028, 0.41068, 0.61964, 0.28351, 0.26726],\n",
       "        [0.70609, 0.46865, 0.51936, 0.67692, 0.55108],\n",
       "        [0.26259, 0.61499, 0.67639, 0.42443, 0.5337 ],\n",
       "        [0.41346, 0.38209, 0.31301, 0.41142, 0.55029],\n",
       "        [0.52923, 0.57243, 0.61173, 0.42836, 0.56581],\n",
       "        [0.6749 , 0.46762, 0.52491, 0.544  , 0.5083 ],\n",
       "        [0.62155, 0.47965, 0.36806, 0.54129, 0.78704],\n",
       "        [0.67672, 0.59937, 0.7278 , 0.01957, 0.33839],\n",
       "        [0.6454 , 0.43846, 0.52846, 0.6505 , 0.63416],\n",
       "        [0.6956 , 0.58142, 0.98014, 0.39097, 0.57164],\n",
       "        [0.66939, 0.36624, 0.70529, 0.45734, 0.79663],\n",
       "        [0.61203, 0.55862, 0.42543, 0.45385, 0.45694],\n",
       "        [0.61141, 0.46949, 0.49447, 0.46084, 0.6768 ],\n",
       "        [0.60634, 0.4732 , 0.60304, 0.48705, 0.36156],\n",
       "        [0.71152, 0.76661, 0.75297, 0.69426, 0.54525],\n",
       "        [0.57352, 0.51757, 0.57435, 0.32291, 0.66185],\n",
       "        [0.72312, 0.53066, 0.61078, 0.38929, 0.47273],\n",
       "        [0.857  , 0.33231, 0.22576, 0.42405, 0.68177],\n",
       "        [0.44605, 0.36043, 0.45727, 0.64763, 0.3344 ],\n",
       "        [0.72087, 0.48411, 0.86355, 0.42488, 0.69379],\n",
       "        [0.58957, 0.32585, 0.25396, 0.38914, 0.2301 ],\n",
       "        [0.55562, 0.35434, 0.3556 , 0.4488 , 0.3704 ],\n",
       "        [0.73482, 0.22209, 0.64068, 0.38988, 0.62982],\n",
       "        [0.5833 , 0.33146, 0.60195, 0.30369, 0.     ],\n",
       "        [0.50656, 0.34516, 0.48441, 0.41739, 0.3529 ],\n",
       "        [0.4714 , 0.41146, 0.81531, 0.42472, 0.36212],\n",
       "        [0.15701, 0.32234, 0.438  , 0.21576, 0.21995],\n",
       "        [0.62235, 0.35121, 0.39769, 0.5716 , 0.47038],\n",
       "        [0.29303, 0.55097, 0.58279, 0.34946, 0.46949],\n",
       "        [0.32305, 0.248  , 0.25575, 0.31567, 0.50589],\n",
       "        [0.44563, 0.46003, 0.4418 , 0.29196, 0.44185],\n",
       "        [0.55962, 0.38785, 0.40341, 0.42092, 0.38721],\n",
       "        [0.57741, 0.3937 , 0.32788, 0.42959, 0.64334],\n",
       "        [0.55841, 0.48252, 0.66247, 0.19249, 0.26879],\n",
       "        [0.51459, 0.33102, 0.44563, 0.50583, 0.62917],\n",
       "        [0.53369, 0.44405, 1.     , 0.32407, 0.42527],\n",
       "        [0.57768, 0.28076, 0.54366, 0.37369, 0.59317],\n",
       "        [0.48792, 0.40029, 0.32594, 0.27221, 0.33398],\n",
       "        [0.41031, 0.35346, 0.3802 , 0.35359, 0.54942],\n",
       "        [0.47421, 0.3864 , 0.46121, 0.40216, 0.29161],\n",
       "        [0.58911, 0.5536 , 0.51632, 0.5054 , 0.34074],\n",
       "        [0.46409, 0.42437, 0.41735, 0.27187, 0.50096],\n",
       "        [0.60223, 0.43812, 0.47492, 0.28082, 0.37271],\n",
       "        [0.76262, 0.29   , 0.01481, 0.326  , 0.5888 ]]),\n",
       " 'y': array([[ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.],\n",
       "        [14.],\n",
       "        [15.],\n",
       "        [16.],\n",
       "        [17.],\n",
       "        [18.],\n",
       "        [19.],\n",
       "        [20.],\n",
       "        [21.],\n",
       "        [22.],\n",
       "        [23.],\n",
       "        [24.],\n",
       "        [25.],\n",
       "        [ 0.],\n",
       "        [ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.],\n",
       "        [ 7.],\n",
       "        [ 8.],\n",
       "        [ 9.],\n",
       "        [10.],\n",
       "        [11.],\n",
       "        [12.],\n",
       "        [13.],\n",
       "        [14.],\n",
       "        [15.],\n",
       "        [16.],\n",
       "        [17.],\n",
       "        [18.],\n",
       "        [19.],\n",
       "        [20.],\n",
       "        [21.],\n",
       "        [22.],\n",
       "        [23.],\n",
       "        [24.],\n",
       "        [25.]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading data into relevant np arrays\n",
    "\n",
    "# specify the location of the data\n",
    "notebook_path = os.path.abspath(\"BRIAN_DIEHL_COOK_2015.ipynb\")\n",
    "#MNIST_data_path = os.path.join(os.path.dirname(notebook_path), \"Datasets\\\\\")\n",
    "touch_data_vec_path = os.path.join(os.path.dirname(notebook_path), \"Datasets\\\\Vector-data-of-A-Z\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "def get_labeled_touch_vec_data(first_file_num, last_file_num, data_path):\n",
    "    \n",
    "    N = last_file_num - (first_file_num - 1)\n",
    "    tot_N = N * 26\n",
    "    x = np.zeros((tot_N, 5))\n",
    "    y = np.zeros((tot_N, 1))\n",
    "    \n",
    "    pos = 0\n",
    "    for file_num in range(first_file_num, last_file_num+1):\n",
    "        file_name = 'Training' + str(file_num) + '.txt'\n",
    "        file_path = data_path + file_name\n",
    "        touch_vecs = open(file_path)\n",
    "        for letter in range(26):\n",
    "            y[letter + pos] = (ord(touch_vecs.read(1)) - 65)\n",
    "            \n",
    "            touch_vecs.read(1)\n",
    "        x[pos:(pos+26), :] = np.transpose(np.loadtxt(file_path, skiprows=1))\n",
    "        \n",
    "        \n",
    "        pos += 26\n",
    "    data = {'x': x, 'y': y}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------ \n",
    "# functions (from original for the most part)\n",
    "#------------------------------------------------------------------------------     \n",
    "\n",
    "def get_labeled_data(picklename, bTrain = True):\n",
    "    \"\"\"Read input-vector (image) and target class (label, 0-9) and return\n",
    "       it as list of tuples.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('%s.pickle' % picklename):\n",
    "        data = pickle.load(open('%s.pickle' % picklename))\n",
    "    else:\n",
    "        # Open the images with gzip in read binary mode\n",
    "        if bTrain:\n",
    "            images = open(MNIST_data_path + 'train-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 'train-labels.idx1-ubyte','rb')\n",
    "        else:\n",
    "            images = open(MNIST_data_path + 't10k-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 't10k-labels.idx1-ubyte','rb')\n",
    "        # Get metadata for images\n",
    "        images.read(4)  # skip the magic_number\n",
    "        number_of_images = unpack('>I', images.read(4))[0]\n",
    "        rows = unpack('>I', images.read(4))[0]\n",
    "        cols = unpack('>I', images.read(4))[0]\n",
    "        # Get metadata for labels\n",
    "        labels.read(4)  # skip the magic_number\n",
    "        N = unpack('>I', labels.read(4))[0]\n",
    "    \n",
    "        if number_of_images != N:\n",
    "            raise Exception('number of labels did not match the number of images')\n",
    "        # Get the data\n",
    "        x = np.zeros((N, rows, cols), dtype=np.uint8)  # Initialize numpy array\n",
    "        y = np.zeros((N, 1), dtype=np.uint8)  # Initialize numpy array\n",
    "        for i in range(N):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: %i\" % i)\n",
    "            x[i] = [[unpack('>B', images.read(1))[0] for unused_col in range(cols)]  for unused_row in range(rows) ]\n",
    "            y[i] = unpack('>B', labels.read(1))[0]\n",
    "            \n",
    "        data = {'x': x, 'y': y, 'rows': rows, 'cols': cols}\n",
    "        pickle.dump(data, open(\"%s.pickle\" % picklename, \"wb\"))\n",
    "    return data\n",
    "\n",
    "def get_matrix_from_file(fileName):\n",
    "    offset = len(ending) + 4\n",
    "    if fileName[-4-offset] == 'X':\n",
    "        n_src = n_input                \n",
    "    else:\n",
    "        if fileName[-3-offset]=='e':\n",
    "            n_src = n_e\n",
    "        else:\n",
    "            n_src = n_i\n",
    "    if fileName[-1-offset]=='e':\n",
    "        n_tgt = n_e\n",
    "    else:\n",
    "        n_tgt = n_i\n",
    "    readout = np.load(fileName)\n",
    "    print(readout.shape, fileName)\n",
    "    value_arr = np.zeros((n_src, n_tgt))\n",
    "    if not readout.shape == (0,):\n",
    "        value_arr[np.int32(readout[:,0]), np.int32(readout[:,1])] = readout[:,2]\n",
    "    return value_arr\n",
    "\n",
    "\n",
    "def save_connections(ending = ''):\n",
    "    print('save connections')\n",
    "    for connName in save_conns:\n",
    "        connMatrix = connections[connName][:]\n",
    "#         connListSparse = ([(i,j[0],j[1]) for i in range(connMatrix.shape[0]) for j in zip(connMatrix.rowj[i],connMatrix.rowdata[i])])\n",
    "        connListSparse = ([(i,j,connMatrix[i,j]) for i in range(connMatrix.shape[0]) for j in range(connMatrix.shape[1]) ])\n",
    "        np.save(data_path + 'weights/' + connName + ending, connListSparse)\n",
    "\n",
    "def save_theta(ending = ''):\n",
    "    print('save theta')\n",
    "    for pop_name in population_names:\n",
    "        np.save(data_path + 'weights/theta_' + pop_name + ending, neuron_groups[pop_name + 'e'].theta)\n",
    "\n",
    "def normalize_weights():\n",
    "    for connName in connections:\n",
    "        if connName[1] == 'e' and connName[3] == 'e':\n",
    "            connection = connections[connName][:]\n",
    "            temp_conn = np.copy(connection)\n",
    "            colSums = np.sum(temp_conn, axis = 0)\n",
    "            colFactors = weight['ee_input']/colSums\n",
    "            for j in range(n_e):#\n",
    "                connection[:,j] *= colFactors[j]\n",
    "            \n",
    "def get_2d_input_weights():\n",
    "    name = 'XeAe'\n",
    "    weight_matrix = np.zeros((n_input, n_e))\n",
    "    n_e_sqrt = int(np.sqrt(n_e))\n",
    "    n_in_sqrt = int(np.sqrt(n_input))\n",
    "    num_values_col = n_e_sqrt*n_in_sqrt\n",
    "    num_values_row = num_values_col\n",
    "    rearranged_weights = np.zeros((num_values_col, num_values_row))\n",
    "    connMatrix = connections[name][:]\n",
    "    weight_matrix = np.copy(connMatrix)\n",
    "        \n",
    "    for i in range(n_e_sqrt):\n",
    "        for j in range(n_e_sqrt):\n",
    "                rearranged_weights[i*n_in_sqrt : (i+1)*n_in_sqrt, j*n_in_sqrt : (j+1)*n_in_sqrt] = \\\n",
    "                    weight_matrix[:, i + j*n_e_sqrt].reshape((n_in_sqrt, n_in_sqrt))\n",
    "    return rearranged_weights\n",
    "\n",
    "\n",
    "def plot_2d_input_weights():\n",
    "    name = 'XeAe'\n",
    "    weights = get_2d_input_weights()\n",
    "    fig = b.figure(fig_num, figsize = (18, 18))\n",
    "    im2 = b.imshow(weights, interpolation = \"nearest\", vmin = 0, vmax = wmax_ee, cmap = cmap.get_cmap('hot_r'))\n",
    "    b.colorbar(im2)\n",
    "    b.title('weights of connection' + name)\n",
    "    fig.canvas.draw()\n",
    "    return im2, fig\n",
    "    \n",
    "def update_2d_input_weights(im, fig):\n",
    "    weights = get_2d_input_weights()\n",
    "    im.set_array(weights)\n",
    "    fig.canvas.draw()\n",
    "    return im\n",
    "\n",
    "def get_current_performance(performance, current_example_num):\n",
    "    current_evaluation = int(current_example_num/update_interval)\n",
    "    start_num = current_example_num - update_interval\n",
    "    end_num = current_example_num\n",
    "    difference = outputNumbers[start_num:end_num, 0] - input_numbers[start_num:end_num]\n",
    "    correct = len(np.where(difference == 0)[0])\n",
    "    performance[current_evaluation] = correct / float(update_interval) * 100\n",
    "    return performance\n",
    "\n",
    "def plot_performance(fig_num):\n",
    "    num_evaluations = int(num_examples/update_interval)\n",
    "    time_steps = range(0, num_evaluations)\n",
    "    performance = np.zeros(num_evaluations)\n",
    "    fig = b.figure(fig_num, figsize = (5, 5))\n",
    "    fig_num += 1\n",
    "    ax = fig.add_subplot(111)\n",
    "    im2, = ax.plot(time_steps, performance) #my_cmap\n",
    "    b.ylim(ymax = 100)\n",
    "    b.title('Classification performance')\n",
    "    fig.canvas.draw()\n",
    "    return im2, performance, fig_num, fig\n",
    "\n",
    "def update_performance_plot(im, performance, current_example_num, fig):\n",
    "    performance = get_current_performance(performance, current_example_num)\n",
    "    im.set_ydata(performance)\n",
    "    fig.canvas.draw()\n",
    "    return im, performance\n",
    "    \n",
    "def get_recognized_number_ranking(assignments, spike_rates):\n",
    "    summed_rates = [0] * 10\n",
    "    num_assignments = [0] * 10\n",
    "    for i in range(10):\n",
    "        num_assignments[i] = len(np.where(assignments == i)[0])\n",
    "        if num_assignments[i] > 0:\n",
    "            summed_rates[i] = np.sum(spike_rates[assignments == i]) / num_assignments[i]\n",
    "    return np.argsort(summed_rates)[::-1]\n",
    "\n",
    "def get_new_assignments(result_monitor, input_numbers):\n",
    "    assignments = np.zeros(n_e)\n",
    "    input_nums = np.asarray(input_numbers)\n",
    "    maximum_rate = [0] * n_e    \n",
    "    for j in range(10):\n",
    "        num_assignments = len(np.where(input_nums == j)[0])\n",
    "        if num_assignments > 0:\n",
    "            rate = np.sum(result_monitor[input_nums == j], axis = 0) / num_assignments\n",
    "        for i in range(n_e):\n",
    "            if rate[i] > maximum_rate[i]:\n",
    "                maximum_rate[i] = rate[i]\n",
    "                assignments[i] = j\n",
    "    return assignments\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6a3522b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to load training set: 0.11454010009765625\n",
      "time needed to load test set: 0.005983829498291016\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------ \n",
    "# load touch vec data\n",
    "#------------------------------------------------------------------------------\n",
    "start = time.time()\n",
    "training = get_labeled_touch_vec_data(1,19,touch_data_vec_path)\n",
    "end = time.time()\n",
    "print('time needed to load training set:', end - start)\n",
    " \n",
    "start = time.time()\n",
    "testing = get_labeled_touch_vec_data(20,20,touch_data_vec_path)\n",
    "end = time.time()\n",
    "print('time needed to load test set:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fae07fa7",
   "metadata": {},
   "outputs": [
    {
     "ename": "EquationError",
     "evalue": "Error parsing the unit specification for variable 'timer': Unit specification refers to 'ms', but this is not a base unit. Use 'second' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\equations\\equations.py:404\u001b[0m, in \u001b[0;36mparse_string_equations\u001b[1;34m(eqns)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     dims, var_type \u001b[38;5;241m=\u001b[39m \u001b[43mdimensions_and_type_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43meq_content\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\equations\\equations.py:326\u001b[0m, in \u001b[0;36mdimensions_and_type_from_string\u001b[1;34m(unit_string)\u001b[0m\n\u001b[0;32m    325\u001b[0m         base_unit \u001b[38;5;241m=\u001b[39m Unit(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39mbase_unit\u001b[38;5;241m.\u001b[39mdim)\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnit specification refers to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, but this is not a base \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_unit\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m     )\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;66;03m# Not a known unit\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unit specification refers to 'ms', but this is not a base unit. Use 'second' instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mEquationError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 153\u001b[0m\n\u001b[0;32m    149\u001b[0m result_monitor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((update_interval,n_e))\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# I simply removed \"compile = True\" and \"freeze=True\" from the parameter lists below \u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m neuron_groups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNeuronGroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_e\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpopulation_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuron_eqs_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv_thresh_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefractory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrefrac_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscr_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m neuron_groups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mNeuronGroup(n_i\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(population_names), neuron_eqs_i, threshold\u001b[38;5;241m=\u001b[39m v_thresh_i, refractory\u001b[38;5;241m=\u001b[39m refrac_i, reset\u001b[38;5;241m=\u001b[39m v_reset_i)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\groups\\neurongroup.py:570\u001b[0m, in \u001b[0;36mNeuronGroup.__init__\u001b[1;34m(self, N, model, method, method_options, threshold, reset, refractory, events, namespace, dtype, dt, clock, order, name, codeobj_class)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m##### Prepare and validate equations\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 570\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mEquations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Equations):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel has to be a string or an Equations \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject, is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\equations\\equations.py:619\u001b[0m, in \u001b[0;36mEquations.__init__\u001b[1;34m(self, eqns, **kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, eqns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eqns, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_equations \u001b[38;5;241m=\u001b[39m \u001b[43mparse_string_equations\u001b[49m\u001b[43m(\u001b[49m\u001b[43meqns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m         \u001b[38;5;66;03m# Do a basic check for the identifiers\u001b[39;00m\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_identifiers()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\utils\\caching.py:107\u001b[0m, in \u001b[0;36mcached.<locals>.cached_func\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     func\u001b[38;5;241m.\u001b[39m_cache_statistics\u001b[38;5;241m.\u001b[39mmisses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 107\u001b[0m     func\u001b[38;5;241m.\u001b[39m_cache[cache_key] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m_cache[cache_key]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\brian2\\equations\\equations.py:406\u001b[0m, in \u001b[0;36mparse_string_equations\u001b[1;34m(eqns)\u001b[0m\n\u001b[0;32m    404\u001b[0m     dims, var_type \u001b[38;5;241m=\u001b[39m dimensions_and_type_from_string(eq_content[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EquationError(\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError parsing the unit specification for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    411\u001b[0m expression \u001b[38;5;241m=\u001b[39m eq_content\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;66;03m# Replace multiple whitespaces (arising from joining multiline\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# strings) with single space\u001b[39;00m\n",
      "\u001b[1;31mEquationError\u001b[0m: Error parsing the unit specification for variable 'timer': Unit specification refers to 'ms', but this is not a base unit. Use 'second' instead."
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------ \n",
    "# set parameters and equations\n",
    "#------------------------------------------------------------------------------\n",
    "test_mode = False\n",
    "\n",
    "### Depracated method of setting preferences in brian1\n",
    "\n",
    "# b.set_global_preferences( \n",
    "#                         defaultclock = b.Clock(dt=0.5*b.ms), # The default clock to use if none is provided or defined in any enclosing scope.\n",
    "#                         useweave = True, # Defines whether or not functions should use inlined compiled C code where defined.\n",
    "#                         gcc_options = ['-ffast-math -march=native'],  # Defines the compiler switches passed to the gcc compiler. \n",
    "#                         #For gcc versions 4.2+ we recommend using -march=native. By default, the -ffast-math optimizations are turned on \n",
    "#                         usecodegen = True,  # Whether or not to use experimental code generation support.\n",
    "#                         usecodegenweave = True,  # Whether or not to use C with experimental code generation support.\n",
    "#                         usecodegenstateupdate = True,  # Whether or not to use experimental code generation support on state updaters.\n",
    "#                         usecodegenthreshold = False,  # Whether or not to use experimental code generation support on thresholds.\n",
    "#                         usenewpropagate = True,  # Whether or not to use experimental new C propagation functions.\n",
    "#                         usecstdp = True,  # Whether or not to use experimental new C STDP.\n",
    "#                        ) \n",
    "\n",
    "### Setting preferences in brian2 (likely nothing needs to be set)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "data_path = './'\n",
    "if test_mode:\n",
    "    weight_path = data_path + 'weights/'\n",
    "    num_examples = 26\n",
    "    use_testing_set = True\n",
    "    do_plot_performance = False\n",
    "    record_spikes = True\n",
    "    ee_STDP_on = False\n",
    "    update_interval = num_examples\n",
    "else:\n",
    "    weight_path = data_path + 'random/'  \n",
    "    num_examples = (19 * 26) * 30 #Thirty repetitions with the data (arbitrary choice) \n",
    "    use_testing_set = False\n",
    "    do_plot_performance = True\n",
    "    #Huh? record spikes true nonetheless, what's the point\n",
    "    if num_examples <= (19 * 26):    \n",
    "        record_spikes = True\n",
    "    else:\n",
    "        record_spikes = True\n",
    "    ee_STDP_on = True\n",
    "\n",
    "\n",
    "ending = ''\n",
    "n_input = 5\n",
    "n_e = 400\n",
    "n_i = n_e \n",
    "single_example_time = 0.35 * b.second #\n",
    "resting_time = 0.15 * b.second\n",
    "runtime = num_examples * (single_example_time + resting_time)\n",
    "if num_examples <= 100:    \n",
    "    update_interval = num_examples\n",
    "    weight_update_interval = 20\n",
    "else:\n",
    "    update_interval = 100\n",
    "    weight_update_interval = 1 ## I randomly just decreased all these numbers by two orders of magnitude\n",
    "if num_examples <= 494:    \n",
    "    save_connections_interval = 100\n",
    "else:\n",
    "    save_connections_interval = 100\n",
    "    update_interval = 100\n",
    "\n",
    "v_rest_e = -65. * b.mV \n",
    "v_rest_i = -60. * b.mV \n",
    "v_reset_e = -65. * b.mV\n",
    "v_reset_i = -45. * b.mV\n",
    "v_thresh_e = -52. * b.mV\n",
    "v_thresh_i = -40. * b.mV\n",
    "refrac_e = 5. * b.ms\n",
    "refrac_i = 2. * b.ms\n",
    "\n",
    "conn_structure = 'dense'\n",
    "weight = {}\n",
    "delay = {}\n",
    "input_population_names = ['X']\n",
    "population_names = ['A']\n",
    "input_connection_names = ['XA']\n",
    "save_conns = ['XeAe']\n",
    "input_conn_names = ['ee_input'] \n",
    "recurrent_conn_names = ['ei', 'ie']\n",
    "weight['ee_input'] = 78.\n",
    "delay['ee_input'] = (0*b.ms,10*b.ms)\n",
    "delay['ei_input'] = (0*b.ms,5*b.ms)\n",
    "input_intensity = 64.\n",
    "start_input_intensity = input_intensity\n",
    "\n",
    "tc_pre_ee = 20*b.ms\n",
    "tc_post_1_ee = 20*b.ms\n",
    "tc_post_2_ee = 40*b.ms\n",
    "nu_ee_pre =  0.0001      # learning rate\n",
    "nu_ee_post = 0.01       # learning rate\n",
    "wmax_ee = 1.0\n",
    "exp_ee_pre = 0.2\n",
    "exp_ee_post = exp_ee_pre\n",
    "STDP_offset = 0.4\n",
    "\n",
    "if test_mode:\n",
    "    scr_e = 'v = v_reset_e; timer = 0*ms'\n",
    "else:\n",
    "    tc_theta = 1e7 * b.ms\n",
    "    theta_plus_e = 0.05 * b.mV\n",
    "    scr_e = 'v = v_reset_e; theta += theta_plus_e; timer = 0*ms'\n",
    "offset = 20.0*b.mV\n",
    "v_thresh_e = '(v>(theta - offset + ' + str(v_thresh_e) + ')) * (timer>refrac_e)'\n",
    "\n",
    "\n",
    "neuron_eqs_e = '''\n",
    "        dv/dt = ((v_rest_e - v) + (I_synE+I_synI) / nS) / (100*ms)  : volt\n",
    "        I_synE = ge * nS *         -v                           : amp\n",
    "        I_synI = gi * nS * (-100.*mV-v)                          : amp\n",
    "        dge/dt = -ge/(1.0*ms)                                   : 1\n",
    "        dgi/dt = -gi/(2.0*ms)                                  : 1\n",
    "        '''\n",
    "if test_mode:\n",
    "    neuron_eqs_e += '\\n  theta      :volt'\n",
    "else:\n",
    "    neuron_eqs_e += '\\n  dtheta/dt = -theta / (tc_theta)  : volt'\n",
    "neuron_eqs_e += '\\n  dtimer/dt = 100.0  : ms'\n",
    "\n",
    "neuron_eqs_i = '''\n",
    "        dv/dt = ((v_rest_i - v) + (I_synE+I_synI) / nS) / (10*ms)  : volt\n",
    "        I_synE = ge * nS *         -v                           : amp\n",
    "        I_synI = gi * nS * (-85.*mV-v)                          : amp\n",
    "        dge/dt = -ge/(1.0*ms)                                   : 1\n",
    "        dgi/dt = -gi/(2.0*ms)                                  : 1\n",
    "        '''\n",
    "eqs_stdp_ee = '''\n",
    "                post2before                            : 1.0\n",
    "                dpre/dt   =   -pre/(tc_pre_ee)         : 1.0\n",
    "                dpost1/dt  = -post1/(tc_post_1_ee)     : 1.0\n",
    "                dpost2/dt  = -post2/(tc_post_2_ee)     : 1.0\n",
    "            '''\n",
    "eqs_stdp_pre_ee = 'pre = 1.; w -= nu_ee_pre * post1'\n",
    "eqs_stdp_post_ee = 'post2before = post2; w += nu_ee_post * pre * post2before; post1 = 1.; post2 = 1.'\n",
    "    \n",
    "b.ion()\n",
    "fig_num = 1\n",
    "neuron_groups = {}\n",
    "input_groups = {}\n",
    "connections = {}\n",
    "stdp_methods = {}\n",
    "rate_monitors = {}\n",
    "spike_monitors = {}\n",
    "spike_counters = {}\n",
    "result_monitor = np.zeros((update_interval,n_e))\n",
    "\n",
    "\n",
    "# I simply removed \"compile = True\" and \"freeze=True\" from the parameter lists below \n",
    "neuron_groups['e'] = b.NeuronGroup(n_e*len(population_names), neuron_eqs_e, threshold= v_thresh_e, refractory= refrac_e, reset= scr_e)\n",
    "\n",
    "neuron_groups['i'] = b.NeuronGroup(n_i*len(population_names), neuron_eqs_i, threshold= v_thresh_i, refractory= refrac_i, reset= v_reset_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc34ca24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 3: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 197\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m#------------------------------------------------------------------------------ \u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# load MNIST\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m#------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    196\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 197\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mget_labeled_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMNIST_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime needed to load training set:\u001b[39m\u001b[38;5;124m'\u001b[39m, end \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m, in \u001b[0;36mget_labeled_data\u001b[1;34m(picklename, bTrain)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"Read input-vector (image) and target class (label, 0-9) and return\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m   it as list of tuples.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m picklename):\n\u001b[1;32m---> 30\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpicklename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Open the images with gzip in read binary mode\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bTrain:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 3: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------ \n",
    "# create network population and recurrent connections\n",
    "#------------------------------------------------------------------------------ \n",
    "for name in population_names:\n",
    "    print('create neuron group', name)\n",
    "    \n",
    "    neuron_groups[name+'e'] = neuron_groups['e'].subgroup(n_e)\n",
    "    neuron_groups[name+'i'] = neuron_groups['i'].subgroup(n_i)\n",
    "    \n",
    "    neuron_groups[name+'e'].v = v_rest_e - 40. * b.mV\n",
    "    neuron_groups[name+'i'].v = v_rest_i - 40. * b.mV\n",
    "    if test_mode or weight_path[-8:] == 'weights/':\n",
    "        neuron_groups['e'].theta = np.load(weight_path + 'theta_' + name + ending + '.npy')\n",
    "    else:\n",
    "        neuron_groups['e'].theta = np.ones((n_e)) * 20.0*b.mV\n",
    "    \n",
    "    print('create recurrent connections')\n",
    "    for conn_type in recurrent_conn_names:\n",
    "        connName = name+conn_type[0]+name+conn_type[1]\n",
    "        weightMatrix = get_matrix_from_file(weight_path + '../random/' + connName + ending + '.npy')\n",
    "        connections[connName] = b.Connection(neuron_groups[connName[0:2]], neuron_groups[connName[2:4]], structure= conn_structure, \n",
    "                                                    state = 'g'+conn_type[0])\n",
    "        connections[connName].connect(neuron_groups[connName[0:2]], neuron_groups[connName[2:4]], weightMatrix)\n",
    "                \n",
    "    if ee_STDP_on:\n",
    "        if 'ee' in recurrent_conn_names:\n",
    "            stdp_methods[name+'e'+name+'e'] = b.STDP(connections[name+'e'+name+'e'], eqs=eqs_stdp_ee, pre = eqs_stdp_pre_ee, \n",
    "                                                           post = eqs_stdp_post_ee, wmin=0., wmax= wmax_ee)\n",
    "\n",
    "    print('create monitors for', name)\n",
    "    rate_monitors[name+'e'] = b.PopulationRateMonitor(neuron_groups[name+'e'], bin = (single_example_time+resting_time)/b.second)\n",
    "    rate_monitors[name+'i'] = b.PopulationRateMonitor(neuron_groups[name+'i'], bin = (single_example_time+resting_time)/b.second)\n",
    "    spike_counters[name+'e'] = b.SpikeCounter(neuron_groups[name+'e'])\n",
    "    \n",
    "    if record_spikes:\n",
    "        spike_monitors[name+'e'] = b.SpikeMonitor(neuron_groups[name+'e'])\n",
    "        spike_monitors[name+'i'] = b.SpikeMonitor(neuron_groups[name+'i'])\n",
    "\n",
    "if record_spikes:\n",
    "    b.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    b.ion()\n",
    "    b.subplot(211)\n",
    "    b.raster_plot(spike_monitors['Ae'], refresh=1000*b.ms, showlast=1000*b.ms)\n",
    "    b.subplot(212)\n",
    "    b.raster_plot(spike_monitors['Ai'], refresh=1000*b.ms, showlast=1000*b.ms)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# create input population and connections from input populations \n",
    "#------------------------------------------------------------------------------ \n",
    "pop_values = [0,0,0]\n",
    "for i,name in enumerate(input_population_names):\n",
    "    input_groups[name+'e'] = b.PoissonGroup(n_input, 0)\n",
    "    rate_monitors[name+'e'] = b.PopulationRateMonitor(input_groups[name+'e'], bin = (single_example_time+resting_time)/b.second)\n",
    "\n",
    "for name in input_connection_names:\n",
    "    print('create connections between', name[0], 'and', name[1])\n",
    "    for connType in input_conn_names:\n",
    "        connName = name[0] + connType[0] + name[1] + connType[1]\n",
    "        weightMatrix = get_matrix_from_file(weight_path + connName + ending + '.npy')\n",
    "        connections[connName] = b.Connection(input_groups[connName[0:2]], neuron_groups[connName[2:4]], structure= conn_structure, \n",
    "                                                    state = 'g'+connType[0], delay=True, max_delay=delay[connType][1])\n",
    "        connections[connName].connect(input_groups[connName[0:2]], neuron_groups[connName[2:4]], weightMatrix, delay=delay[connType])\n",
    "     \n",
    "    if ee_STDP_on:\n",
    "        print('create STDP for connection', name[0]+'e'+name[1]+'e')\n",
    "        stdp_methods[name[0]+'e'+name[1]+'e'] = b.STDP(connections[name[0]+'e'+name[1]+'e'], eqs=eqs_stdp_ee, pre = eqs_stdp_pre_ee, \n",
    "                                                       post = eqs_stdp_post_ee, wmin=0., wmax= wmax_ee)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# run the simulation and set inputs\n",
    "#------------------------------------------------------------------------------ \n",
    "previous_spike_count = np.zeros(n_e)\n",
    "assignments = np.zeros(n_e)\n",
    "input_numbers = [0] * num_examples\n",
    "outputNumbers = np.zeros((num_examples, 10))\n",
    "if not test_mode:\n",
    "    input_weight_monitor, fig_weights = plot_2d_input_weights()\n",
    "    fig_num += 1\n",
    "if do_plot_performance:\n",
    "    performance_monitor, performance, fig_num, fig_performance = plot_performance(fig_num)\n",
    "for i,name in enumerate(input_population_names):\n",
    "    input_groups[name+'e'].rate = 0\n",
    "b.run(0)\n",
    "j = 0\n",
    "while j < (int(num_examples)):\n",
    "    if test_mode:\n",
    "        if use_testing_set:\n",
    "            rates = testing['x'][j%100,:,:].reshape((n_input)) *  input_intensity\n",
    "        else:\n",
    "            rates = training['x'][j%494,:,:].reshape((n_input)) *  input_intensity\n",
    "    else:\n",
    "        normalize_weights()\n",
    "        rates = training['x'][j%494,:,:].reshape((n_input)) *  input_intensity\n",
    "    input_groups['Xe'].rate = rates\n",
    "#     print('run number:', j+1, 'of', int(num_examples))\n",
    "    b.run(single_example_time, report='text')\n",
    "            \n",
    "    if j % update_interval == 0 and j > 0:\n",
    "        assignments = get_new_assignments(result_monitor[:], input_numbers[j-update_interval : j])\n",
    "    if j % weight_update_interval == 0 and not test_mode:\n",
    "        update_2d_input_weights(input_weight_monitor, fig_weights)\n",
    "    if j % save_connections_interval == 0 and j > 0 and not test_mode:\n",
    "        save_connections(str(j))\n",
    "        save_theta(str(j))\n",
    "    \n",
    "    current_spike_count = np.asarray(spike_counters['Ae'].count[:]) - previous_spike_count\n",
    "    previous_spike_count = np.copy(spike_counters['Ae'].count[:])\n",
    "    if np.sum(current_spike_count) < 5:\n",
    "        input_intensity += 1\n",
    "        for i,name in enumerate(input_population_names):\n",
    "            input_groups[name+'e'].rate = 0\n",
    "        b.run(resting_time)\n",
    "    else:\n",
    "        result_monitor[j%update_interval,:] = current_spike_count\n",
    "        if test_mode and use_testing_set:\n",
    "            input_numbers[j] = testing['y'][j%100][0]\n",
    "        else:\n",
    "            input_numbers[j] = training['y'][j%494][0]\n",
    "        outputNumbers[j,:] = get_recognized_number_ranking(assignments, result_monitor[j%update_interval,:])\n",
    "        if j % 100 == 0 and j > 0:\n",
    "            print('runs done:', j, 'of', int(num_examples))\n",
    "        if j % update_interval == 0 and j > 0:\n",
    "            if do_plot_performance:\n",
    "                unused, performance = update_performance_plot(performance_monitor, performance, j, fig_performance)\n",
    "                print('Classification performance', performance[:(j/float(update_interval))+1])\n",
    "        for i,name in enumerate(input_population_names):\n",
    "            input_groups[name+'e'].rate = 0\n",
    "        b.run(resting_time)\n",
    "        input_intensity = start_input_intensity\n",
    "        j += 1\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# save results\n",
    "#------------------------------------------------------------------------------ \n",
    "print('save results')\n",
    "if not test_mode:\n",
    "    save_theta()\n",
    "if not test_mode:\n",
    "    save_connections()\n",
    "else:\n",
    "    np.save(data_path + 'activity/resultPopVecs' + str(num_examples), result_monitor)\n",
    "    np.save(data_path + 'activity/inputNumbers' + str(num_examples), input_numbers)\n",
    "    \n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# plot results\n",
    "#------------------------------------------------------------------------------ \n",
    "if rate_monitors:\n",
    "    b.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    for i, name in enumerate(rate_monitors):\n",
    "        b.subplot(len(rate_monitors), 1, i)\n",
    "        b.plot(rate_monitors[name].times/b.second, rate_monitors[name].rate, '.')\n",
    "        b.title('Rates of population ' + name)\n",
    "    \n",
    "if spike_monitors:\n",
    "    b.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    for i, name in enumerate(spike_monitors):\n",
    "        b.subplot(len(spike_monitors), 1, i)\n",
    "        b.raster_plot(spike_monitors[name])\n",
    "        b.title('Spikes of population ' + name)\n",
    "        \n",
    "if spike_counters:\n",
    "    b.figure(fig_num)\n",
    "    fig_num += 1\n",
    "    for i, name in enumerate(spike_counters):\n",
    "        b.subplot(len(spike_counters), 1, i)\n",
    "        b.plot(spike_counters['Ae'].count[:])\n",
    "        b.title('Spike count of population ' + name)\n",
    "\n",
    "plot_2d_input_weights()\n",
    "b.ioff()\n",
    "b.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATOR file, figure all this after figuring the first part\n",
    "\n",
    "\n",
    "'''\n",
    "Created on 15.12.2014\n",
    "\n",
    "@author: Peter U. Diehl\n",
    "'''\n",
    "\n",
    "import brian as b\n",
    "from brian import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.cm as cmap\n",
    "import time\n",
    "import os.path\n",
    "import scipy \n",
    "import cPickle as pickle\n",
    "from struct import unpack\n",
    "import brian.experimental.realtime_monitor as rltmMon\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# functions\n",
    "#------------------------------------------------------------------------------     \n",
    "def get_labeled_data(picklename, bTrain = True):\n",
    "    \"\"\"Read input-vector (image) and target class (label, 0-9) and return\n",
    "       it as list of tuples.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('%s.pickle' % picklename):\n",
    "        data = pickle.load(open('%s.pickle' % picklename))\n",
    "    else:\n",
    "        # Open the images with gzip in read binary mode\n",
    "        if bTrain:\n",
    "            images = open(MNIST_data_path + 'train-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 'train-labels.idx1-ubyte','rb')\n",
    "        else:\n",
    "            images = open(MNIST_data_path + 't10k-images.idx3-ubyte','rb')\n",
    "            labels = open(MNIST_data_path + 't10k-labels.idx1-ubyte','rb')\n",
    "        # Get metadata for images\n",
    "        images.read(4)  # skip the magic_number\n",
    "        number_of_images = unpack('>I', images.read(4))[0]\n",
    "        rows = unpack('>I', images.read(4))[0]\n",
    "        cols = unpack('>I', images.read(4))[0]\n",
    "        # Get metadata for labels\n",
    "        labels.read(4)  # skip the magic_number\n",
    "        N = unpack('>I', labels.read(4))[0]\n",
    "        if number_of_images != N:\n",
    "            raise Exception('number of labels did not match the number of images')\n",
    "        # Get the data\n",
    "        x = np.zeros((N, rows, cols), dtype=np.uint8)  # Initialize numpy array\n",
    "        y = np.zeros((N, 1), dtype=np.uint8)  # Initialize numpy array\n",
    "        for i in xrange(N):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"i: %i\" % i)\n",
    "            x[i] = [[unpack('>B', images.read(1))[0] for unused_col in xrange(cols)]  for unused_row in xrange(rows) ]\n",
    "            y[i] = unpack('>B', labels.read(1))[0]\n",
    "        data = {'x': x, 'y': y, 'rows': rows, 'cols': cols}\n",
    "        pickle.dump(data, open(\"%s.pickle\" % picklename, \"wb\"))\n",
    "    return data\n",
    "\n",
    "def get_recognized_number_ranking(assignments, spike_rates):\n",
    "    summed_rates = [0] * 10\n",
    "    num_assignments = [0] * 10\n",
    "    for i in xrange(10):\n",
    "        num_assignments[i] = len(np.where(assignments == i)[0])\n",
    "        if num_assignments[i] > 0:\n",
    "            summed_rates[i] = np.sum(spike_rates[assignments == i]) / num_assignments[i]\n",
    "    return np.argsort(summed_rates)[::-1]\n",
    "\n",
    "def get_new_assignments(result_monitor, input_numbers):\n",
    "    print result_monitor.shape\n",
    "    assignments = np.ones(n_e) * -1 # initialize them as not assigned\n",
    "    input_nums = np.asarray(input_numbers)\n",
    "    maximum_rate = [0] * n_e    \n",
    "    for j in xrange(10):\n",
    "        num_inputs = len(np.where(input_nums == j)[0])\n",
    "        if num_inputs > 0:\n",
    "            rate = np.sum(result_monitor[input_nums == j], axis = 0) / num_inputs\n",
    "        for i in xrange(n_e):\n",
    "            if rate[i] > maximum_rate[i]:\n",
    "                maximum_rate[i] = rate[i]\n",
    "                assignments[i] = j \n",
    "    return assignments\n",
    "\n",
    "MNIST_data_path = './'\n",
    "data_path = './activity/'\n",
    "training_ending = '100'\n",
    "testing_ending = '100'\n",
    "start_time_training = 0\n",
    "end_time_training = int(training_ending)\n",
    "start_time_testing = 0\n",
    "end_time_testing = int(testing_ending)\n",
    "\n",
    "n_e = 400\n",
    "n_input = 784\n",
    "ending = ''\n",
    "\n",
    "print 'load MNIST'\n",
    "training = get_labeled_data(MNIST_data_path + 'training')\n",
    "testing = get_labeled_data(MNIST_data_path + 'testing', bTrain = False)\n",
    "\n",
    "print 'load results'\n",
    "training_result_monitor = np.load(data_path + 'resultPopVecs' + training_ending + ending + '.npy')\n",
    "training_input_numbers = np.load(data_path + 'inputNumbers' + training_ending + '.npy')\n",
    "testing_result_monitor = np.load(data_path + 'resultPopVecs' + testing_ending + '.npy')\n",
    "testing_input_numbers = np.load(data_path + 'inputNumbers' + testing_ending + '.npy')\n",
    "print training_result_monitor.shape\n",
    "\n",
    "print 'get assignments'\n",
    "test_results = np.zeros((10, end_time_testing-start_time_testing))\n",
    "test_results_max = np.zeros((10, end_time_testing-start_time_testing))\n",
    "test_results_top = np.zeros((10, end_time_testing-start_time_testing))\n",
    "test_results_fixed = np.zeros((10, end_time_testing-start_time_testing))\n",
    "assignments = get_new_assignments(training_result_monitor[start_time_training:end_time_training], \n",
    "                                  training_input_numbers[start_time_training:end_time_training])\n",
    "print assignments\n",
    "counter = 0 \n",
    "num_tests = end_time_testing / 100\n",
    "sum_accurracy = [0] * num_tests\n",
    "while (counter < num_tests):\n",
    "    end_time = min(end_time_testing, 100*(counter+1))\n",
    "    start_time = 100*counter\n",
    "    test_results = np.zeros((10, end_time-start_time))\n",
    "    print 'calculate accuracy for sum'\n",
    "    for i in xrange(end_time - start_time):\n",
    "        test_results[:,i] = get_recognized_number_ranking(assignments, \n",
    "                                                          testing_result_monitor[i+start_time,:])\n",
    "    difference = test_results[0,:] - testing_input_numbers[start_time:end_time]\n",
    "    correct = len(np.where(difference == 0)[0])\n",
    "    incorrect = np.where(difference != 0)[0]\n",
    "    sum_accurracy[counter] = correct/float(end_time-start_time) * 100\n",
    "    print 'Sum response - accuracy: ', sum_accurracy[counter], ' num incorrect: ', len(incorrect)\n",
    "    counter += 1\n",
    "print 'Sum response - accuracy --> mean: ', np.mean(sum_accurracy),  '--> standard deviation: ', np.std(sum_accurracy)\n",
    "\n",
    "\n",
    "b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random connection generating file\n",
    "'''\n",
    "Created on 15.12.2014\n",
    "\n",
    "@author: Peter U. Diehl\n",
    "'''\n",
    "\n",
    "import scipy.ndimage as sp\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "\n",
    "def randomDelay(minDelay, maxDelay):\n",
    "    return np.random.rand()*(maxDelay-minDelay) + minDelay\n",
    "        \n",
    "        \n",
    "def computePopVector(popArray):\n",
    "    size = len(popArray)\n",
    "    complex_unit_roots = np.array([np.exp(1j*(2*np.pi/size)*cur_pos) for cur_pos in xrange(size)])\n",
    "    cur_pos = (np.angle(np.sum(popArray * complex_unit_roots)) % (2*np.pi)) / (2*np.pi)\n",
    "    return cur_pos\n",
    "\n",
    "        \n",
    "def sparsenMatrix(baseMatrix, pConn):\n",
    "    weightMatrix = np.zeros(baseMatrix.shape)\n",
    "    numWeights = 0\n",
    "    numTargetWeights = baseMatrix.shape[0] * baseMatrix.shape[1] * pConn\n",
    "    weightList = [0]*int(numTargetWeights)\n",
    "    while numWeights < numTargetWeights:\n",
    "        idx = (np.int32(np.random.rand()*baseMatrix.shape[0]), np.int32(np.random.rand()*baseMatrix.shape[1]))\n",
    "        if not (weightMatrix[idx]):\n",
    "            weightMatrix[idx] = baseMatrix[idx]\n",
    "            weightList[numWeights] = (idx[0], idx[1], baseMatrix[idx])\n",
    "            numWeights += 1\n",
    "    return weightMatrix, weightList\n",
    "        \n",
    "    \n",
    "def create_weights():\n",
    "    \n",
    "    nInput = 784\n",
    "    nE = 400\n",
    "    nI = nE \n",
    "    dataPath = './random/'\n",
    "    weight = {}\n",
    "    weight['ee_input'] = 0.3 \n",
    "    weight['ei_input'] = 0.2 \n",
    "    weight['ee'] = 0.1\n",
    "    weight['ei'] = 10.4\n",
    "    weight['ie'] = 17.0\n",
    "    weight['ii'] = 0.4\n",
    "    pConn = {}\n",
    "    pConn['ee_input'] = 1.0 \n",
    "    pConn['ei_input'] = 0.1 \n",
    "    pConn['ee'] = 1.0\n",
    "    pConn['ei'] = 0.0025\n",
    "    pConn['ie'] = 0.9\n",
    "    pConn['ii'] = 0.1\n",
    "    \n",
    "    \n",
    "    print 'create random connection matrices'\n",
    "    connNameList = ['XeAe']\n",
    "    for name in connNameList:\n",
    "        weightMatrix = np.random.random((nInput, nE)) + 0.01\n",
    "        weightMatrix *= weight['ee_input']\n",
    "        if pConn['ee_input'] < 1.0:\n",
    "            weightMatrix, weightList = sparsenMatrix(weightMatrix, pConn['ee_input'])\n",
    "        else:\n",
    "            weightList = [(i, j, weightMatrix[i,j]) for j in xrange(nE) for i in xrange(nInput)]\n",
    "        np.save(dataPath+name, weightList)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print 'create connection matrices from E->I which are purely random'\n",
    "    connNameList = ['XeAi']\n",
    "    for name in connNameList:\n",
    "        weightMatrix = np.random.random((nInput, nI))\n",
    "        weightMatrix *= weight['ei_input']\n",
    "        weightMatrix, weightList = sparsenMatrix(weightMatrix, pConn['ei_input'])\n",
    "        print 'save connection matrix', name\n",
    "        np.save(dataPath+name, weightList)\n",
    "        \n",
    "    \n",
    "    \n",
    "    print 'create connection matrices from E->I which are purely random'\n",
    "    connNameList = ['AeAi']\n",
    "    for name in connNameList:\n",
    "        if nE == nI:\n",
    "            weightList = [(i, i, weight['ei']) for i in xrange(nE)]\n",
    "        else:\n",
    "            weightMatrix = np.random.random((nE, nI))\n",
    "            weightMatrix *= weight['ei']\n",
    "            weightMatrix, weightList = sparsenMatrix(weightMatrix, pConn['ei'])\n",
    "        print 'save connection matrix', name\n",
    "        np.save(dataPath+name, weightList)\n",
    "        \n",
    "        \n",
    "        \n",
    "    print 'create connection matrices from I->E which are purely random'\n",
    "    connNameList = ['AiAe']\n",
    "    for name in connNameList:\n",
    "        if nE == nI:\n",
    "            weightMatrix = np.ones((nI, nE))\n",
    "            weightMatrix *= weight['ie']\n",
    "            for i in xrange(nI):\n",
    "                weightMatrix[i,i] = 0\n",
    "            weightList = [(i, j, weightMatrix[i,j]) for i in xrange(nI) for j in xrange(nE)]\n",
    "        else:\n",
    "            weightMatrix = np.random.random((nI, nE))\n",
    "            weightMatrix *= weight['ie']\n",
    "            weightMatrix, weightList = sparsenMatrix(weightMatrix, pConn['ie'])\n",
    "        print 'save connection matrix', name\n",
    "        np.save(dataPath+name, weightList)\n",
    "    \n",
    "         \n",
    "if __name__ == \"__main__\":\n",
    "    create_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
